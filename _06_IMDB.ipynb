{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_06_IMDB.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOoFPNyNfWk2ioOcY03D8/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bltsezer/Dataset_Repo/blob/main/_06_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlSSTDqTqEOf"
      },
      "source": [
        "IMDB veriseti indir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmBml8cLqUb0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uteWuTpp7BA"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIqaffOpqJnG"
      },
      "source": [
        "!pwd\n",
        "dataset_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avAXWfpFqabm"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "print(os.listdir(dataset_dir))\n",
        "print(os.listdir(train_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3Wxg6xrqlUw"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-qp6n4cr-sv"
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NpHnWNXthHw"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='training', \n",
        "    seed=seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17kwpoRjt0ht"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(\"Review: \", text_batch.numpy()[i])\n",
        "    print(\"Label: \", label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4_oluvouO_K"
      },
      "source": [
        "print(\"Label 0 \", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 \", raw_train_ds.class_names[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12oYAXQQueI-"
      },
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j4Hr_m3un2d"
      },
      "source": [
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk7_nSyJvNVV"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D7ML6xVvcll"
      },
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWWP1d7VwDRo"
      },
      "source": [
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWbhkXGtwWda"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "\n",
        "\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"\\nLabel\", raw_train_ds.class_names[first_label])\n",
        "print(\"\\nVectorized review\", vectorize_text(first_review, first_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA_C9O-5wdv4"
      },
      "source": [
        "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
        "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X-qJGn2xJRx"
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7l0wYiWxO3i"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaB-45RaxSC3"
      },
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyeQeGfCxmcf"
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVQPbWeRxzdc"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lp1nfhnx-rp"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNxlska4yAoe"
      },
      "source": [
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNvKBgQVyLu-"
      },
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a8vFnN-yTKx"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLwkcdYLycnD"
      },
      "source": [
        "examples = [\n",
        "  \"The movie was great!\",\n",
        "  \"The movie was okay.\",\n",
        "  \"The movie was terrible...\"\n",
        "]\n",
        "\n",
        "export_model.predict(examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lSgAt2CytiG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}